"C_black",
"C_union",
"C_income",
"C_incomedkna",
"C_educ",
"pre_hhincomeforecast",
"post_hhincomeforecast",
"pre_nationaleconforecast",
"post_nationaleconforecast",
"delta_holidayspend",
"delta_vacationspend",
"pre_happy",
"post_happy",
"pre_stateeconforecast",
"post_stateeconforecast")
labs <- labs.all[match(vars, vars.all)]
# summary statistics
wt <- gerb$wt
meannum <- function(x) mean(as.numeric(x), na.rm=T)
sdnum <- function(x) (mean(as.numeric(x)^2, na.rm=T) - mean(as.numeric(x), na.rm=T)^2)^.5
wtmeannum <- function(x) weighted.mean(as.numeric(x),wt, na.rm=T)
wtmeannum2 <- function(x) weighted.mean(as.numeric(x),gerb$weight, na.rm=T)
wtsdnum <- function(x) (weighted.mean(as.numeric(x)^2,wt, na.rm=T) - weighted.mean(as.numeric(x),wt, na.rm=T)^2)^.5
minnum <- function(x) min(as.numeric(x), na.rm=T)
maxnum <- function(x) max(as.numeric(x), na.rm=T)
nomsampmean <- apply(gerb[,vars], 2, meannum)
nomsampsd <- apply(gerb[,vars], 2, sdnum)
effsampmean <- apply(gerb[,vars], 2, wtmeannum)
effsampsd <- apply(gerb[,vars], 2, wtsdnum)
sumstats <- round(100*cbind(nomsampmean,
nomsampsd,
effsampmean,
effsampsd))/100
rownames(sumstats) <- labs
colnames(sumstats) <- c("Mean",
"S.D.",
"Mean",
"S.D.")
xtable(sumstats)
world <-  readShapePoly("world_countries_boundary_file_world_2002.shp")
jensen.cc <- read.dta("jensen-rep.dta")
X.vars <- c(	"var5",
"market",
"lgdppc",
"gdpgrowt",
"tradeofg",
"overallb",
"generalg",
"country",
"d2",
"d3")
X.vars.f <- paste(X.vars,collapse="+")
fit.y <- lm(as.formula(paste("Fvar5~regime+", X.vars.f, sep="")), data=jensen.cc)
fit.d <- lm(as.formula(paste("regime~", X.vars.f, sep="")), data=jensen.cc)
summary(fit.y)
summary(fit.d)
d.tilde <- as.numeric(residuals(fit.d))
w <- d.tilde^2
w1 <- tapply(w, jensen.cc$country, mean)
mapnames <-  read.csv("mapnames_filled.csv")
mapnames$weight <- 0
mapnames$weight[match(rownames(w1), mapnames$jensen)] <- as.numeric(w1)
attributes(world)$data$weight <- 0
attributes(world)$data$weight[match(mapnames$mapname,attributes(world)$data$NAME)] <- mapnames$weight
attributes(world)$data$incl <- 0
attributes(world)$data$incl[match(mapnames$mapname,attributes(world)$data$NAME)] <- as.numeric(!is.na(mapnames$jensen))
plot(	world,
col=gray(1-.75*attributes(world)$data$incl),
border="gray", lwd=.25)
plot(	world,
col=gray(1-abs(attributes(world)$data$weight)/max(abs(attributes(world)$data$weight))),
border="gray", lwd=.25)
attributes(world)$data
attributes(world)$data$weight
sum(attributes(world)$data$weight)
sort(attributes(world)$data$weight, decreasing = T)
plot(sort(attributes(world)$data$weight, decreasing = T))
sum(w)
plot(sort(w, decreasing = T))
rm(list=ls())
library(foreign)
library(xtable)
gerb <- read.dta("gerber-huber-analysis-data.dta")
fit <- lm(delta_vacationspend~pre_pid5
+C_age
+C_age2
+C_female
+C_hispanic
+C_black
+C_union
+C_income
+C_incomedkna
+C_educ, data=gerb)
# change wd as needed:
setwd("C:/Users/Kevin/Documents/GitHub/causal_inference23/code/a_s2016")
library(foreign)
library(maptools)
world <-  readShapePoly("world_countries_boundary_file_world_2002.shp")
jensen.cc <- read.dta("jensen-rep.dta")
X.vars <- c(	"var5",
"market",
"lgdppc",
"gdpgrowt",
"tradeofg",
"overallb",
"generalg",
"country",
"d2",
"d3")
X.vars.f <- paste(X.vars,collapse="+")
fit.y <- lm(as.formula(paste("Fvar5~regime+", X.vars.f, sep="")), data=jensen.cc)
fit.d <- lm(as.formula(paste("regime~", X.vars.f, sep="")), data=jensen.cc)
summary(fit.y)
summary(fit.d)
summary(jensen.cc$regime)
library(tidyverse)
library(fixest)
install.packages("fixest")
library(fixest)
df <- haven::read_dta("https://raw.github.com/scunning1975/mixtape/master/thornton_hiv.dta")
# the `with` command lets you access variables in `df` without
# doing `df$` a bunch
with(df, {
mean(got[any == 1], na.rm = TRUE) - mean(got[any == 0], na.rm = TRUE)
})
View(df)
# Cluster SEs by village
feols(got ~ i(any), data = df, cluster = ~villnum)
feols(
c(male, age, hiv2004, educ2004, land2004, usecondom04) ~
i(any) + tinc + i(under) + i(rumphi) + i(balaka),
data = df, cluster = ~villnum
) |>
etable()
# high incentive
feols(
got ~ i(any),
data = df |> filter(tinc >= 2 | tinc == 0), cluster = ~villnum
)
# low incentive
feols(
got ~ i(any),
data = df |> filter(tinc <= 1 | tinc == 0), cluster = ~villnum
)
# linear dose-response curve
feols(
got ~ i(any) + tinc,
data = df, cluster = ~villnum
)
df <- df |> filter(!is.na(any))
permuteHIV <- function(df, random = TRUE) {
# Shuffle `any`
if(random == TRUE) {
df$any <- sample(df$any, replace = FALSE)
}
# `with` lets you access variables in tb without doing tb$ a bunch
ate <- with(df, {
mean(got[any == 1], na.rm = TRUE) - mean(got[any == 0], na.rm = TRUE)
})
return(ate)
}
# Observed treatment effect
permuteHIV(df, random = FALSE)
n_iterations <- 1000
# Run iterations
ate <- c(permuteHIV(df, random = FALSE))
for(i in 1:(n_iterations - 1)) {
ate <- c(ate, permuteHIV(df, random = TRUE))
}
# Histogram of placebo effects
hist(ate)
#calculating the p-value
ate = abs(ate)
obs_te = ate[1]
# `ecdf` gives us the empirical CDF of `ate`
empirical_cdf = ecdf(ate)
# percentile of obs_te
obs_percentile = empirical_cdf(obs_te)
# p-value
1 - obs_percentile
obs_percentile
rm(list=ls())
# change wd as needed:
setwd("C:/Users/Kevin/Documents/GitHub/causal_inference23/code/a_s2016")
library(foreign)
library(maptools)
world <-  readShapePoly("world_countries_boundary_file_world_2002.shp")
jensen.cc <- read.dta("jensen-rep.dta")
View(jensen.cc)
X.vars <- c(	"var5",
"market",
"lgdppc",
"gdpgrowt",
"tradeofg",
"overallb",
"generalg",
"country",
"d2",
"d3")
X.vars.f <- paste(X.vars,collapse="+")
X.vars.f
fit.y <- lm(as.formula(paste("Fvar5~regime+", X.vars.f, sep="")), data=jensen.cc)
fit.d <- lm(as.formula(paste("regime~", X.vars.f, sep="")), data=jensen.cc)
summary(fit.y)
summary(fit.d)
d.tilde <- as.numeric(residuals(fit.d))
w <- d.tilde^2
w1 <- tapply(w, jensen.cc$country, mean)
mapnames <-  read.csv("mapnames_filled.csv")
rownames(w1)
mapnames$weight[match(rownames(w1), mapnames$jensen)] <- as.numeric(w1)
View(mapnames)
attributes(world)$data$weight <- 0
attributes(world)$data$weight[match(mapnames$mapname,attributes(world)$data$NAME)] <- mapnames$weight
attributes(world)$data$incl <- 0
attributes(world)$data$incl[match(mapnames$mapname,attributes(world)$data$NAME)] <- as.numeric(!is.na(mapnames$jensen))
plot(	world,
col=gray(1-.75*attributes(world)$data$incl),
border="gray", lwd=.25)
pdf(file="jensen-nominal-map.pdf", height=5, width=8)
plot(	world,
col=gray(1-.75*attributes(world)$data$incl),
border="gray", lwd=.25)
dev.off()
plot(	world,
col=gray(1-abs(attributes(world)$data$weight)/max(abs(attributes(world)$data$weight))),
border="gray", lwd=.25)
attributes(world)$data$weight <- 0
attributes(world)$data$weight[match(mapnames$mapname,attributes(world)$data$NAME)] <- mapnames$weight
attributes(world)$data$incl <- 0
attributes(world)$data$incl[match(mapnames$mapname,attributes(world)$data$NAME)] <- as.numeric(!is.na(mapnames$jensen))
plot(	world,
col=gray(1-abs(attributes(world)$data$weight)/max(abs(attributes(world)$data$weight))),
border="gray", lwd=.25)
w1
jensen.cc$w<-w
library(dplyer)
library(dplyr)
?order
x<-jensen.cc[order(jensen.cc$w, decreasing = T),]
View(x)
xx<-x[i:length(x$country),]
#### WHat is the minimum number of individual observations
####you have to delete in order to eliminate the significant result?
i<-1
xx<-x[i:length(x$country),]
#### WHat is the minimum number of individual observations
####you have to delete in order to eliminate the significant result?
i<-2
xx<-x[i:length(x$country),]
fit.y <- lm(as.formula(paste("Fvar5~regime+", X.vars.f, sep="")), data=xx)
summary(fit.y)
fit.y <- lm(as.formula(paste("Fvar5~regime+", X.vars.f, sep="")), data=jensen.cc)
summary(fit.y)
#### WHat is the minimum number of individual observations
####you have to delete in order to eliminate the significant result?
i<-10
xx<-x[i:length(x$country),]
fit.y <- lm(as.formula(paste("Fvar5~regime+", X.vars.f, sep="")), data=xx)
summary(fit.y)
#### WHat is the minimum number of individual observations
####you have to delete in order to eliminate the significant result?
i<-100
xx<-x[i:length(x$country),]
fit.y <- lm(as.formula(paste("Fvar5~regime+", X.vars.f, sep="")), data=xx)
summary(fit.y)
#### WHat is the minimum number of individual observations
####you have to delete in order to eliminate the significant result?
i<-20
xx<-x[i:length(x$country),]
fit.y <- lm(as.formula(paste("Fvar5~regime+", X.vars.f, sep="")), data=xx)
fit.y <- lm(as.formula(paste("Fvar5~regime+", X.vars.f, sep="")), data=xx)
summary(fit.y)
#### WHat is the minimum number of individual observations
####you have to delete in order to eliminate the significant result?
i<-50
xx<-x[i:length(x$country),]
fit.y <- lm(as.formula(paste("Fvar5~regime+", X.vars.f, sep="")), data=xx)
summary(fit.y)
#### WHat is the minimum number of individual observations
####you have to delete in order to eliminate the significant result?
i<-100
xx<-x[i:length(x$country),]
fit.y <- lm(as.formula(paste("Fvar5~regime+", X.vars.f, sep="")), data=xx)
summary(fit.y)
new<-runif(1630, 0,20)
jensen.cc$new<-runif(1630, 0,20)
?rnum
?runif
?samp
?sample
jensen.cc$indicator<-sample.int(1,1630)
jensen.cc$indicator<-sample.int(1,1630, replace = T)
jensen.cc$indicator<-sample.int(2,1630, replace = T)
jensen.cc$new[jensen.cc$indicator==2]<-jensen.cc$regime
jensen.cc$new[jensen.cc$indicator==2,]<-jensen.cc$regime
jensen.cc$regime
jensen.cc$indicator<-sample.int(2,1630, replace = T)
jensen.cc$new[jensen.cc$indicator==2]
jensen.cc$new[jensen.cc$indicator==2]<-jensen.cc$regime[jensen.cc$indicator==2]
View(jensen.cc)
#### WHat is the minimum number of individual observations
####you have to delete in order to eliminate the significant result?
i<-15
xx<-x[i:length(x$country),]
fit.y <- lm(as.formula(paste("Fvar5~regime+", X.vars.f, sep="")), data=xx)
summary(fit.y)
fit.y <- lm(as.formula(paste("Fvar5~regime+new+", X.vars.f, sep="")), data=jensen.cc)
summary(fit.y)
library(tidyverse)
library(fixest)
library(haven)
library(MatchIt)
# Experimental data
df_exp <- haven::read_dta("https://raw.github.com/scunning1975/mixtape/master/nsw_mixtape.dta")
# Baseline covariate balance
feols(
c(re74, re75, marr, educ, age, black, hisp) ~ i(treat),
data = df_exp, vcov = "hc1"
) |>
etable()
# Estimate treatment effect
feols(re78 ~ i(treat), data = df_exp, vcov = "hc1")
df_cps <- haven::read_dta("https://raw.github.com/scunning1975/mixtape/master/cps_mixtape.dta")
# Treated experimental units with CPS units as controls
df_nonexp <- bind_rows(df_exp |> filter(treat == 1), df_cps)
df_nonexp |>
feols(re78 ~ i(treat), vcov = "hc1")
df_nonexp <- df_nonexp |>
mutate(
agesq = age^2,
agecube = age^3,
educsq = educ^2,
u74 = (re74 == 0),
u75 = (re75 == 0)
)
logit_nsw <- feglm(
treat ~ age + agesq + agecube + educ + educsq +
marr + nodegree + black + hisp + re74 +
re75 + u74 + u75,
family = binomial(link = "logit"),
data = df_nonexp
)
df_nonexp$pscore <- predict(logit_nsw, type = "response")
# Weights are implicitly normalized when using `feols`,
# plus it gives standard errors
df_nonexp |>
feols(re78 ~ i(treat),
weights = ~inv_ps_weight, vcov = "hc1"
)
# inverse propensity score weights
df_nonexp <- df_nonexp |>
mutate(
# ATT
# ATE
inv_ps_weight = treat / pscore + (1-treat) * 1/(1-pscore)
# ATC
)
# Weights are implicitly normalized when using `feols`,
# plus it gives standard errors
df_nonexp |>
feols(re78 ~ i(treat),
weights = ~inv_ps_weight, vcov = "hc1"
)
# Normalized weights
df_nonexp |>
filter(pscore > 0.1 & pscore < 0.9) |>
feols(re78 ~ i(treat),
weights = ~inv_ps_weight, vcov = "hc1"
)
# Weights are implicitly normalized when using `feols`,
# plus it gives standard errors
df_nonexp |>
feols(re78 ~ i(treat),
weights = ~inv_ps_weight, vcov = "hc1"
)
# Normalized weights
df_nonexp |>
filter(pscore > 0.1 & pscore < 0.9) |>
feols(re78 ~ i(treat),
weights = ~inv_ps_weight, vcov = "hc1"
)
# Weights are implicitly normalized when using `feols`,
# plus it gives standard errors
df_nonexp |>
feols(re78 ~ i(treat),
weights = ~inv_ps_weight, vcov = "hc1"
)
# 1:1 nearest neighbor matching with replacement on
# the Mahalanobis distance
nn_out <- matchit(
treat ~ age + agesq + agecube + educ + educsq +
marr + nodegree + black + hisp + re74 +
re75 + u74 + u75,
data = df_nonexp, distance = "mahalanobis",
replace = TRUE, estimand = "ATT"
)
df_nonexp$nn_weights = nn_out$weights
feols(
re78 ~ i(treat), weights = ~nn_weights,
data = df_nonexp,  vcov = "hc1"
)
cem_out <- matchit(
treat ~ age + agesq + agecube + educ + educsq +
marr + nodegree + black + hisp + re74 +
re75 + u74 + u75,
data = df_nonexp,
method = "cem", estimand = "ATT"
)
df_nonexp$cem_weights = cem_out$weights
feols(
re78 ~ i(treat), weights = ~cem_weights,
data = df_nonexp,  vcov = "hc1"
)
df_nonexp |>
feols(re78 ~ i(treat), vcov = "hc1")
# Baseline covariate balance
feols(
c(re74, re75, marr, educ, age, black, hisp) ~ i(treat),
data = df_exp, vcov = "hc1"
) |>
etable()
library(tidyverse)
library(fixest)
library(haven)
library(MatchIt)
# Experimental data
df_exp <- haven::read_dta("https://raw.github.com/scunning1975/mixtape/master/nsw_mixtape.dta")
# Baseline covariate balance
feols(
c(re74, re75, marr, educ, age, black, hisp) ~ i(treat),
data = df_exp, vcov = "hc1"
) |>
etable()
?feols
# Estimate treatment effect
feols(re78 ~ i(treat), data = df_exp, vcov = "hc1")
df_cps <- haven::read_dta("https://raw.github.com/scunning1975/mixtape/master/cps_mixtape.dta")
# Treated experimental units with CPS units as controls
df_nonexp <- bind_rows(df_exp |> filter(treat == 1), df_cps)
df_nonexp |>
feols(re78 ~ i(treat), vcov = "hc1")
df_nonexp <- df_nonexp |>
mutate(
agesq = age^2,
agecube = age^3,
educsq = educ^2,
u74 = (re74 == 0),
u75 = (re75 == 0)
)
logit_nsw <- feglm(
treat ~ age + agesq + agecube + educ + educsq +
marr + nodegree + black + hisp + re74 +
re75 + u74 + u75,
family = binomial(link = "logit"),
data = df_nonexp
)
df_nonexp$pscore <- predict(logit_nsw, type = "response")
# inverse propensity score weights
df_nonexp <- df_nonexp |>
mutate(
# ATT
#inv_ps_weight = treat + (1-treat) * pscore/(1-pscore)
# ATE
inv_ps_weight = treat / pscore + (1-treat) * 1/(1-pscore)
# ATC
# inv_ps_weight = treat * (1-pscore)/pscore - (1-treat)
)
# Weights are implicitly normalized when using `feols`,
# plus it gives standard errors
df_nonexp |>
feols(re78 ~ i(treat),
weights = ~inv_ps_weight, vcov = "hc1"
)
# Normalized weights
df_nonexp |>
filter(pscore > 0.1 & pscore < 0.9) |>
feols(re78 ~ i(treat),
weights = ~inv_ps_weight, vcov = "hc1"
)
# 1:1 nearest neighbor matching with replacement on
# the Mahalanobis distance
nn_out <- matchit(
treat ~ age + agesq + agecube + educ + educsq +
marr + nodegree + black + hisp + re74 +
re75 + u74 + u75,
data = df_nonexp, distance = "mahalanobis",
replace = TRUE, estimand = "ATT"
)
df_nonexp$nn_weights = nn_out$weights
feols(
re78 ~ i(treat), weights = ~nn_weights,
data = df_nonexp,  vcov = "hc1"
)
cem_out <- matchit(
treat ~ age + agesq + agecube + educ + educsq +
marr + nodegree + black + hisp + re74 +
re75 + u74 + u75,
data = df_nonexp,
method = "cem", estimand = "ATT"
)
df_nonexp$cem_weights = cem_out$weights
feols(
re78 ~ i(treat), weights = ~cem_weights,
data = df_nonexp,  vcov = "hc1"
)
# Baseline covariate balance
feols(
c(re74, re75, marr, educ, age, black, hisp) ~ i(treat),
data = df_exp, vcov = "hc1"
) |>
etable()
